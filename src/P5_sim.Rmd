---
title: "Spatial interaction modelling"
author: Emmanouil Tranos
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
editor: source
bibliography: references.bib
---

## Resources

Some of the materials for this tutorial have been adapted from:

-   the [Origin-destination data with stplanr](https://docs.ropensci.org/stplanr/articles/stplanr-od.html) manual,

-   the [Modelling Population Flows Using Spatial Interaction Models](https://rpubs.com/adam_dennett/376877) tutorial, and

-   [@oshan2021spatial](http://openjournals.wu.ac.at/region/paper_175/175.html)

-   [@lovelace2019geocomputation](https://geocompr.robinlovelace.net/)

```{r setup, results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(geojsonio)
library(SpatialPosition)
library(stargazer)
library(rprojroot)
library(kableExtra)
library(sf)
library(caret)

knitr::opts_chunk$set(include=TRUE, message=FALSE, warning=FALSE)

# This is the project path
path <- find_rstudio_root_file()
```

## Current research

As an introduction, see [here](https://etranos.info/post/sad2021/SAD2021.html#/){target="_blank"} 
some a recent research project we did, which used 
spatial interaction type of modelling to predict interregional trade flow, which
web data [@tranos2022using].

## Commuting data and networks

This the same data and the same data preparation we did for the [network analysis practical](https://etranos.info/urban_analytics_city_science/src/P2_network_analysis.html). Please follow the below workflow, which you should already be familiar with. Once the data preperation is done, we will model these commuting flows.

For this tutorial we will use travel to work data from the 2011 UK Census. The data is available [online](http://wicid.ukdataservice.ac.uk/cider/wicid/downloads.php?wicid_Session=742d61be88b0614c3982455e542bc776), but it requires an academic login. After you log in, download the **WU03UK** element, save the .csv on your working directory under a `/data` directory and unzip it. We will use the:

> Location of usual residence and place of work by method of travel to work

for

> Census Merged local authority districts in England and Wales, Council areas in Scotland, Local government districts in Northern Ireland.

The below code loads the data.

```{r data load}

path.data <- paste0(path, "/data/wu03uk_v3/wu03uk_v3.csv")
commuting <- read_csv(path.data)
commuting
```

You might have observe some weird codes (OD0000001, OD0000002, OD0000003 and OD0000004). With some simple Google searching we can find the [2011 Census Origin-Destination Data User Guide](https://www.ons.gov.uk/file?uri=/census/2011census/2011censusdata/originanddestinationdata/secureoriginanddestinationtables/2011censusoduserguide_tcm77-383888.pdf), which indicates that these codes do not refer to local authorities:

-   OD0000001 = Mainly work at or from home

-   OD0000002 = Offshore installation

-   OD0000003 = No fixed place

-   OD0000004 = Outside UK

For the sake of simplicity we will remove these non-geographical nodes.

```{r drop non_la, include=TRUE, results= 'asis', message=FALSE}
non.la <- c("OD0000001", "OD0000002", "OD0000003", "OD0000004")
commuting <- commuting %>% 
  filter(!`Area of workplace` %in% non.la)
```

check the `%in%` [operator](https://stat.ethz.ch/R-manual/R-devel/library/base/html/match.html).

Now let's do some clean-up of the `commuting` data frame. Let's remind ourselves how the data look like

We are keeping the English and Wales local authorities by keeping the observations with a local authority code starting from E (for England) and W (for Wales).

```{r la.cleanup2, include=TRUE, results= 'markup', message=FALSE}
commuting <- commuting %>% filter(startsWith(`Area of usual residence`, "E") |
                                  startsWith(`Area of usual residence`, "W")) %>% 
                           filter(startsWith(`Area of workplace`, "E") |
                                  startsWith(`Area of workplace`, "W")) %>% 
  glimpse()
```

We can also see how many rows we dropped with `glimpse()`.

It is very important to distinguish between intra- and inter-local authority flows. In network analysis terms, these are the values we find on the diagonal of an adjacency matrix and refer to the commuting flows within a specific local authority or between different ones. For this exercise we are dropping the intra-local authority flows. Although not used here, we also create a new object with the intra-local authority flows.

```{r la.cleanup3, include=TRUE, results= 'markup', message=FALSE}
commuting.intra <- commuting %>%
  filter(`Area of usual residence` == `Area of workplace`)
commuting <- commuting %>%
  filter(`Area of usual residence` != `Area of workplace`) %>% 
  glimpse()
```

Please note the constant use of `glimpse()` to keep control of how many observations we have and check if we missed anything.

Also, take a note of the `commuting` object, which includes multiple types of commuting flows. Therefore, we will build $3$ different networks:

1.  one for all the commuting flows

2.  one only for train flows

3.  one only for bicycle flows.

```{r seperate commuting objects, include=TRUE, results= 'markup', message=FALSE}

commuting.all <- commuting %>%
  select(`Area of usual residence`,
                `Area of workplace`,
                `All categories: Method of travel to work`) %>%
  rename(o = `Area of usual residence`,     # Area of usual residence is annoyingly
         d = `Area of workplace`,           # long, so I am renaiming theses columns
         weight = `All categories: Method of travel to work`)

# just FYI this is how you could have achieved the same output using base R
# instead of dplyr of the tidyverse ecosystem
# commuting.all <- commuting[,1:3]
# names(commuting.all)[1] <- "o"
# names(commuting.all)[2] <- "d"
# names(commuting.all)[3] <- "weight"

commuting.train <- commuting %>%
  select(`Area of usual residence`,
         `Area of workplace`,
         `All categories: Method of travel to work`,
         `Train`) %>%
  rename(o = `Area of usual residence`,
         d = `Area of workplace`,
         weight = `All categories: Method of travel to work`) %>%
  # The below code drops all the lines with 0 train flows in order to exclude
  # these edges from the network.
  filter(Train!=0)

commuting.bicycle <- commuting %>%
  select(`Area of usual residence`,
                `Area of workplace`,
                `All categories: Method of travel to work`,
                `Bicycle`) %>%
  rename(o = `Area of usual residence`,
         d = `Area of workplace`,
         weight = `All categories: Method of travel to work`) %>%
  # The below code drops all the lines with 0 bicycle flows in order to exclude
  # these edges from the network.
  filter(Bicycle!=0)
```

Unless you know the local authority codes by hard, it might be useful to also add the corresponding local authority names. These can be easily obtained from the [ONS](https://geoportal.statistics.gov.uk/datasets/ons::cmlad-dec-2011-super-generalised-clipped-boundaries-gb/explore?showTable=true). The below code directly downloads a `GeoJSON` file with the local authorities in England and Wales. If you don't know what a `GeoJSON` file is, have a look [here](https://en.wikipedia.org/wiki/GeoJSON). Boundary data can also be obtained by [UK Data Service](https://borders.ukdataservice.ac.uk/easy_download.html).

For the time being we are only interested in the local authority names and codes. We will use the spatial object later.

**Tip**: the below code downloads the `GeoJSON` file over the web. If you want to run the code multiple times, it might be faster to download the file ones, save it on your hard drive and the point this location to `st_read()`.

```{r la1, include=TRUE, results= 'hide', message=FALSE}
la <-st_read("https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/CMLAD_Dec_2011_SGCB_GB_2022/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson")
glimpse(la) 

la.names <- as.data.frame(la) %>% 
  select(cmlad11cd, cmlad11nm)    # all we need is the LA names and codes
```

## Spatial interaction modelling

Let's now move to model these flows. If you remember the basic Gravity model is defined as following:

$$T_{ij} = k \displaystyle \frac{V_i^\lambda W_j^\alpha}{D_{ij}^\beta}$$

And if we take the logarithms of both sides of the equation we can transform the Gravity model to something which looks like a linear model:

$$lnT_{ij} = lnk + \lambda lnV_i + \alpha lnW_j - \beta lnD_{ij}$$

The above transformed equation can be estimated as a linear model if we assume that $y = lnT_{ij}$, $c = lnk$, $x_1 = lnV_i$, $a_1 = \lambda$ etc. Hence, we can use OLS to estimate the following:

$$lnT_{ij} = lnk + \lambda lnV_i + \alpha lnW_j - \beta lnD_{ij} + e_{ij}$$

This is what is known as the log-linear or log-normal transformation.

There are a number of issues with such an approach though. Most importantly, our dependent variable is not continuous, but instead a discrete, positive variable (there are no flows of -324.56 people!). Therefore we need to employ an appropriate estimator and this is what the Poisson regression does. Briefly, if we exponentiate both sides, the above equation can be written as:

$$T_{ij} = e^{lnk + \lambda lnV_i + \alpha lnW_j - \beta lnD_{ij}}$$ The above is in the form of the Poisson regression. So, we are interested in modelling *not* the mean $T_{ij}$ drawn from a normally distributed $T$, but, instead, the mean $T_{ij}$, which is the average of the all the flows (i.e. counts) between any $i$ and $j$. For more details about the Poisson regression have a look at  [@roback2021beyond](https://bookdown.org/roback/bookdown-bysh/ch-poissonreg.html#initial-models). For this practical we will estimate the commuting flows using both OLS and Poisson regressions.

But before we get into the estimation we need to build a data frame, which includes all the necessary variables. These are the origin-destination flows $T$ between $i$ and $j$, the distance $dist$ between $i$ and $j$ and the characteristics $V$ and $W$ of origin $i$ and destinations $j$ that we believe *push* and *pull* individuals to commute.

```{r si data, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

# we use the `SpatialPosition` package and the `CreateDistMatrix()` function to
# calculate a distance matrix for all local authorities,
la.d <- CreateDistMatrix(la, la, longlat = T)

# we use as column and row name the local authority codes
rownames(la.d) <- la$cmlad11cd
colnames(la.d) <- la$cmlad11cd

# This is a matrix of the distances between *all* local authorities. We then use
# the function as.data.frame.table() to convert this matrix to a data frame each
# line of which represents an origin-destination pair.
la.d <- as.data.frame.table(la.d, responseName = "value")
glimpse(la.d)

# Please note that the elements of the diagonal are present in this distance matrix.
```

If you want to check that the distances we are correct, use google maps to calculate the distance between E41000001 (Middlesbrough) and E41000002 (Hartlepool). Remember that the la.d is expressed in meters.

What is missing here? Do you remember the intra-zone commuting flows? E.g. the commuters that live and work in Bristol? We had removed these from the network analysis and visualisation element. Because we don't have information about the distances of the intra-zone commutes, we will exclude them from this analysis.

```{r si data2, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
la.d <- la.d %>%
  filter(Var1 != Var2) %>% 
  glimpse()
```

What we want to do is to match the data frame with all the distances with the commuting flows data frame. To do that we will (1) create a new data frame for the origin-destination pair and (2) match the two data frames.

```{r si data3, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

la.d <- la.d %>% 
  mutate(ij.code = paste(Var1, Var2, sep = "_")) %>% 
  glimpse()

commuting <- commuting %>% 
  mutate(ij.code = paste(`Area of usual residence`,
                          `Area of workplace`,
                           sep = "_")) %>% 
  glimpse()
```

Before we perform the match, keep a note of how many observations both data frames have in order to check if we loose any observations during the matching. As you can see the commuting data frame has less observations than the la.d one, which includes all the possible origin-destination pairs.

What does it mean? That for some origin-destination pairs there are no commuting flows, which of course makes sense. We need to include these pairs with $flow = 0$ in our data though because the lack of commuting flows **is not** missing data!

```{r si data4, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

commuting.si <- full_join(la.d, commuting, by = "ij.code") %>% 
  glimpse()

# Some variables are repetitive or need name change
commuting.si <- commuting.si %>% 
  rename(i = Var1,
         j = Var2,
         distance = value) %>% 
  select(-`Area of usual residence`,
         -`Area of workplace`) %>% 
  glimpse()

# Let's see if we have any missing values
sapply(commuting.si, function(x) sum(is.na(x)))
```

There are quite a few. What does it mean? That there are no commuting flows for these origin-destination pairs and,therefore, were excluded from the origin commuting data set we downloaded. So, we are going to replace these $NAs$ with $0s$.

```{r si data5, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
commuting.si <- commuting.si %>% 
  replace(., is.na(.), 0)
```

Now let's bring data for some $i$ and $j$ characteristics that we believe that affect commuting. I have prepared such a small data set from the census, which includes resident population and working populations as *push* and *pull* variables. These data have been obtained by [nomis](https://www.nomisweb.co.uk/published/census/odexplorer.asp).

```{r si ij data, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
data.workplace <- read_csv("https://www.dropbox.com/s/0ym88p8quwaiyau/data_workplace.csv?dl=1")

# Dropbox trick: to use in an .Rmd the link that Dropbox provides to share a file
# replace dl=0 with dl=1 at the end of the link

data.resident <- read_csv("https://www.dropbox.com/s/09d7v5cm6ov3ioz/data_resident.csv?dl=1")

commuting.si <- commuting.si %>% 
  left_join(data.resident, by = c('i' = 'Merging.Local.Authority.Code')) %>% 
  left_join(data.workplace, by = c('j' = 'Merging.Local.Authority.Code')) 
  
```

```{block, type='alert alert-warning'}
**Question** are there any redundant columns? Can you remove them?
```

Before we start modelling these flows, let's plot our variables.

```{r si.plots, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
ggplot(commuting.si, aes(x=distance,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)

ggplot(commuting.si, aes(x=resident,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)

ggplot(commuting.si, aes(x=workplace,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)
```

```{block, type='alert alert-warning'}
**Question** What do you take from these graphs?
```

Let's try now to model these flows. We will start with a simple OLS to estimate the above specifications. Please pay attention to the small trick we did. Because there are non-materialised origin-destination pairs (i.e. with 0 flows), we added a small value ($0.5$) in the dependent variable. Otherwise we will receive an error as the logarithm of $0$ is not defined.

```{r si_ols, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

ols.model <- lm(log(`All categories: Method of travel to work`+.5) ~
                  log(distance) + log(resident) + log(workplace),
                data = commuting.si)

# To see the model output you can use the summary() function.
summary(ols.model)
```

So, the OLS regression estimated the four parameters:

-   $lnk = 7.971$
-   $\beta = -1.840$
-   $\lambda = 0.514$
-   $\alpha = -0.852$

Let's estimate now our model using a Poisson regression. Given that we don't take the logarithm of the dependent variable, there is no need to add $0.5$.

```{r si_poisson, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

glm.model <- glm((`All categories: Method of travel to work`)~
                   log(distance) + log(resident) + log(workplace),
                 family = poisson(link = "log"), data = commuting.si)
summary(glm.model)
```

The following parameter1 have been estimated

-   $lnk = 11.635$
-   $\beta = -1.816$
-   $\lambda = 0.319$
-   $\alpha = 0.820$

As you can see the differences are rather small.

The `stargazer` package I use below creates elegant regression tables. Replace `type = "html"` with `type = "text"` to have be able to read the results using the .Rmd document. the "html" option is useful for when knitting the script to an .html document.

```{r stargazer, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
stargazer(ols.model, glm.model, type = "html")
```

```{block, type='alert alert-warning'}
**Question** Can you interpret the regression results?
```

## This is not a Machine Learning introduction...

... but maybe a sneak preview of the philosophy behind *modern* data science approaches to answer computational problems. We are using the `caret` package to:

-   split our data into test and training

-   train a `lm()`and a `glm()` model using the training data set

-   use the estimated models to make *out-of-sample* predictions using the test data set

-   plot the results and the relevant metrics and choose the best model.

This is obviously an oversimplification, but it provides a good preview of machine learning frameworks.

It is worth browsing the [`caret`](https://topepo.github.io/caret/) package.

**The below will take a few minutes!**

```{r predict, eval = TRUE, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
commuting.si <- commuting.si %>% 
  drop_na() 

set.seed(3456)
trainIndex <- createDataPartition(commuting.si$`All categories: Method of travel to work`, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dim(trainIndex)
dim(commuting.si)

commuting.si.train <- commuting.si[ trainIndex,]
commuting.si.test  <- commuting.si[-trainIndex,]

model.lm <- train(`All categories: Method of travel to work` ~
                    distance + resident + workplace,
                    data = commuting.si,
                    method = "lm")

predictions.lm <- predict(model.lm, commuting.si.test)
lm.metrics <- postResample(pred = predictions.lm, obs = commuting.si.test$`All categories: Method of travel to work`) 

model.glm <- train(`All categories: Method of travel to work` ~
                    distance + resident + workplace,
                    data = commuting.si,
                    method = "glmnet",
                    family = "poisson", 
                    na.action = na.omit)

predictions.glm <- predict(model.glm, commuting.si.test)
glm.metrics <- postResample(pred = predictions.glm, obs = commuting.si.test$`All categories: Method of travel to work`) 
bind_rows(lm.metrics, glm.metrics) %>% 
  mutate(Model = c("lm", "glm")) %>% 
  select(Model, Rsquared, RMSE, MAE) %>% 
  kable(digits = 3)
```

```{block, type='alert alert-warning'}
**Question** Which model would you choose?
```

```{r predict.plots, eval = TRUE, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}

predict.lm.plot <- bind_cols(commuting.si.test$`All categories: Method of travel to work`, predictions.lm) %>% 
  rename(y = '...1',
         y_hat = '...2') %>% 
  ggplot(aes(x=y, y=y_hat)) + geom_point() + ggtitle("lm") + 
  theme(plot.title = element_text(hjust = 0.5))

predict.glm.plot <- bind_cols(commuting.si.test$`All categories: Method of travel to work`, predictions.glm) %>% 
  rename(y = '...1',
         y_hat = '...2') %>% 
  ggplot(aes(x=y, y=y_hat)) + geom_point() + ggtitle("glm") + 
  theme(plot.title = element_text(hjust = 0.5))

library(patchwork)
predict.lm.plot + predict.glm.plot

```
