---
title: "Diversity in cities"
author: |
  | 
  | Emmanouil Tranos
  |
  | University of Bristol, Alan Turing Institute 
  | [e.tranos@bristol.ac.uk](mailto:e.tranos@bristol.ac.uk), [\@EmmanouilTranos](https://twitter.com/EmmanouilTranos),  [etranos.info](https://etranos.info/)
format: revealjs
output:
  html_document:
    code_folding: hide
editor: source
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: true
#| results: false
#| include: false

library(rprojroot)

# This is the project path
path <- find_rstudio_root_file()
images.path <- paste0(path, "/images/")
```

## Economic diversity

-   Production, i.e. firms

-   Consumption, i.e. product variety

-   Labour pool, i.e. skills in labour market

In general is *a good thing* for:

-   urban economies

-   productivity

-   urban and industrial agglomeration

## Opposing forces

-   Within-sector or Marshall--Arrow--Romer (MAR) spillovers

-   Between-sector or Jacobs spillovers

-   Large empirical literature trying to identify the optimal ratio, e.g. @saviotti2008export and @caragliu2016both

-   MAR externalities (or spillovers): good for productivity and short-term growth

-   Jacobean externalities: good for innovation and long-term growth

## Opposing forces

Using more clear economics terminology [@fujita1989urban]:

-   Diverse cities (heterogeneous agglomerations) enjoy economies of scope

-   Homogeneous agglomeration enjoy increasing returns from economies of scale

## On the ground

-   Ambiguous concepts

-   Variety, diversity, difference: a relative concept of agglomeration and the clustering of activities

-   Not only higher 'abundance', 'difference' or 'number', but also the degrees of 'richness', 'concentration'
or 'evenness' [@yuo2021environmental]

-   Different ways to measure [@bettencourt2021introduction]

## Spieces richness...

-   ... aka variety

-   $D = \sum_{i}^n p_{i}^0$

-   $p_i$ is the proportion of data points in the $i$th category

-   $n$ is the number of total categories

-   A count of different species / categories / ...

**Interpretation:**

-   Plurality

-   Availability of options

## Shannon entropy

-   $H = -\sum_{i}^n p_{i} \ln{p_{i}}$

-   $n$ is the number of total categories

-   $p_i$ is the proportion of data points in the $i$th category

-   Probably the most common diversity index.

-   **Interpretation:**

    -   If one category dominates ➔ less surprise ➔ low entropy

    -   No category dominates ➔ more surprise ➔ high entropy

## Herfindahl-Hirschman index

-   $HHI = \sum_{i}^{n}(p_{i}^2)$

-   $p_i$ is the proportion of data points in the $i$th category

-   Concentration of the market.

-   **Interpretation:**

    -   $1/n \leq HHI \leq 1$

    -   Two scenarios:

```{r, echo=TRUE}
HHI_1 = .8^2 + .05^2 + .05^2 + .1^2
HHI_1

HHI_2 = .25^2 + .25^2 + .25^2 + .25^2
HHI_2
```

## Herfindahl-Hirschman index

-   Caution: alternative specification

-   $HHI = 1- \sum_{i}^{n}(p_{i}^2)$

# Examples

------------------------------------------------------------------------

![](images/paper_example1.png)

------------------------------------------------------------------------

![](images/paper_example2.png)

## Relatedness

-   *Relatedness* spans the continuum between MAR and Jacobs [@hidalgo2021economic]

-   Related activities are neither exactly the same nor completely different [@frenken2007related; @boschma2012technological]

-   Why? Because:

    -   identical activities compete for customers and resources,

    -   no learning between very dissimilar economic activities

## Relatedness

-   Absorptive capacity: a firm's capacity to absorb new knowledge depends on its prior level of related knowledge [@cohen1990absorptive]

## Economic complexity

-   Large scale fine-grained data on economic activities

-   Learn about abstract factors of production and the way they combine into outputs

-   Dimensionality reduction techniques to data on the geography of activities, e.g. employment by industry or patents by technology

-   Machine learning and network techniques to predict and explain the economic trajectories of countries, cities and regions

For a review, check @hidalgo2021economic and @balland2022new.

## Measuring diversity

<iframe src="https://find-and-update.company-information.service.gov.uk/" width="800" height="400" data-external="1" frameBorder="0" scrolling="yes"></iframe>

<small> Source: [Companies House](https://www.gov.uk/government/organisations/companies-house){target="_blank"} </small>

## Measuring diversity

-   Go to [data.london.gov.uk](https://data.london.gov.uk/dataset/directory-of-london-businesses){target="_blank"}

-   Download and and save locally the `Businesses-in-London.csv`

-   Make sure you know the file location!

-   We will use the `REAT` and `entropy` packages. Check what these packages do [here](https://cran.r-project.org/web/packages/REAT/REAT.pdf){target="_blank"} and
[here](https://cran.r-project.org/web/packages/entropy/entropy.pdf){target="_blank"}.

-   Install them if needed with `install.packages("packagename")`

## Measuring diversity {.scrollable}

```{r, echo=TRUE}

library(tidyverse)  # for data wrangling
library(rprojroot)  # for relative paths
library(REAT)       # for diversity measures
library(entropy)    # for entropy
library(cluster)    # for cluster analysis
library(factoextra) # help functions for clustering 
library(kableExtra) # for nice html tables
library(dbscan)     # for HDBSCAL
library(sf)         # for mapping

# This is the project path
path <- find_rstudio_root_file()
path.data <- paste0(path, "/data/businesses-in-london.csv")

london.firms <- read_csv(path.data) 

london.firms.sum <- london.firms %>% 
  filter(SICCode.SicText_1!="None Supplied") %>% # dropping NAs in essence
  group_by(oslaua, SICCode.SicText_1) %>%        # grouping by Local Authority and SIC code
  summarise(n = n()) %>%                         # summarise: n is the number of firms per Local Authority and SIC code
  mutate(total = sum(n),                         # total equal all firms
         freq = n / total) %>%                   # just a frequency
  group_by(oslaua) %>%                           # grouping again only by Local Authority
  summarise(richness = n_distinct(SICCode.SicText_1), # the number of distinct SIC per Local Authority
            entropy = entropy(freq, method = "ML"),   # entropy for each Local Authority, we did the first group_by() and mutate() to be                                                           able to calculate freq so we can calculate entropy
            herf = herf(n)) %>%                       # HHI for each local authority
  arrange(-herf)                                      # sort based on HHI (descending)

london.firms.sum %>% kbl() %>%
  kable_styling(full_width = F) %>%                   # A nice(r) table
  scroll_box(width = "800px", height = "300px")
```

## Measuring diversity

::: callout-tip
You don't know what local authorities these codes refer to. You should download the codes and names and join them with your data from [here](https://geoportal.statistics.gov.uk/documents/c4f647d8a4a648d7b4a1ebf057f8aaa3/about){target="_blank"}.
:::

::: callout-tip
Discuss what we can learn from this exercise.

Can you think of a way to understand how different these indices are among London's Local Authorities?
:::

## Mapping diversity {.scrollable}

```{r, }
#| label: maps
#| echo: true
#| include: true
#| message: false
#| warning: false
#| error: false
#| results: asis

path.shape <- paste0(path, "/data/Local_Authority_Districts_(May_2021)_UK_BFE.geojson")

london <- st_read(path.shape, quiet = T) %>%
  dplyr::filter(LAD21CD %in% (london.firms$oslaua))

london <- merge(london, london.firms.sum, by.x = "LAD21CD", by.y = "oslaua" )
  
ggplot() +
  geom_sf(data = london, aes(fill = entropy), color = NA) +
  labs(
    title = "Business diversity in London' Local Autorities",
    fill = "Entropy") +
  scale_fill_viridis_c() +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5)) # centres the title
```

## Clustering

-   Reducing the dimensions of the observation space

-   Classification of observations into (exclusive) groups

-   Distance or (dis)similarity between each pair of observations to create a distance or dissimilarity or matrix

-   Observations within the same group are as similar as possible

-   Based on @boehmke2019hands available [here](https://bradleyboehmke.github.io/HOML/){target="_blank"}

-   Plenty of other resources online and in textbooks

------------------------------------------------------------------------

![Source: [medium.com](https://medium.com/@recrosoft.io/supervised-vs-unsupervised-learning-key-differences-cdd46206cdcb)](images/classical_ml.png){target="_blank"}

## Clustering

1.  *k*-means

2.  Hierarchical clustering

## *k*-means

::: r-fit-text
1.  *k* is the number of clusters and is pre-defined

2.  The algorithm selects *k* random observations (starting centres)

3.  The remaining observations are assigned to the nearest centre

4.  Recalculates the new centres

5.  Re-check cluster assignment

6.  Iterative process to minimise *within-cluster variation* until convergence
:::

$SS_{within} = \sum_{k=1}^k W(C_{k}) = \sum_{k=1}^k \sum_{x_i\in C_K}(x_i-\mu_k)^2$

## *k*-means {.scrollable}

First, create an appropriate data frame

```{r, echo = TRUE}

la.sic <- london.firms %>% 
  filter(SICCode.SicText_1!="None Supplied") %>% # Drop firms which haven't declared SIC code
  group_by(oslaua, SICCode.SicText_1) %>%        # Group by Local Authorities and SIC code
  summarise(n = n()) %>%                         # Summarise; n = number of observations
  mutate(total = sum(n),                         # New column: total number of observations
         freq = n / total) %>%                   # New column: frequency
  arrange(oslaua,-n) %>%                         # Just arrange by Local Authority and descenting order of n
  select(-n, -total) %>%                         # Drop n and total, we don't need them any more.
  pivot_wider(names_from = SICCode.SicText_1, values_from = freq) %>% # Data transformation: from long to wide. Have a look: https://tidyr.tidyverse.org/reference/pivot_wider.html
  replace(is.na(.), 0)                          # Replace any missing values with 0 as missing value represent SIC codes with 0 frequency

la.sic %>%  
  select(1:20) %>%  # Select the first 20 columns as there 1037 in total
  kbl() %>%
  kable_styling()   # Nice(r) table
```

## *k*-means {.scrollable}

```{r, echo=TRUE}
kclust = kmeans(la.sic[,-1], centers = 10, nstart = 10) # be aware of the [,-1]
str(kclust)
```

`centers` is 10 x 1036: 1036 is the number of SIC codes.

## Choosing *k*

1.  Rule of thumb: $k = \sqrt{n/2}$

2.  The *elbow* method

    -   Compute *k*-means clustering for different values of *k*

    -   Calculate $SS_{within}$

    -   Plot and spot the loction of a bend

## Choosing *k*

```{r, echo=TRUE}

fviz_nbclust(
  la.sic[,-1], 
  kmeans, 
  k.max = 20,
  method = "wss"
)

```

## Hierarchical clustering

::: columns
::: {.column width="60%"}
![](images/dendrogram.png)

<small> Source: [\@boehmke2019hands](https://bradleyboehmke.github.io/HOML){target="_blank"} </small>
:::

::: {.column width="40%"}
1.  Agglomerative clustering (AGNES -- AGglomerative NESting)

2.  Divisive hierarchical clustering (DIANA -- DIvise ANAlysis)

Dissimilarity (distance) of observations
:::
:::

## Hierarchical clustering

```{r, echo=TRUE}

# distances between observations
d <- dist(la.sic)

# creates labels for the dendrogam
l <- london.firms %>% distinct(oslaua) %>% arrange(oslaua)

hclust = hclust(d)

plot(hclust, hang=-1, labels=l$oslaua, main='Default from hclust') 
#hang: the fraction of the plot height by which labels should hang below the rest of the plot. A negative value will cause the labels to hang down from 0.
```

## Optimal number of clusters

```{r}
p1 <- fviz_nbclust(la.sic, FUN = hcut, 
                   method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(la.sic, FUN = hcut, 
                   method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(la.sic[-1], FUN = hcut, 
                   method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

## Optimal number of clusters

::: callout-tip
Explore what the 2 cluster solution tells us about London?
:::

```{r, eval=FALSE}
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hclust)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = .15)
fviz_dend(dend_cuts$lower[[2]])

sub_grp <- cutree(hclust, k = 2)
table(sub_grp)

fviz_dend(
  hclust,
  k = 2,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco",
  cex = 0.1
)
```

## Clusters in space {.scrollable}

-   Create a SIC frequency table

```{r, echo=TRUE}

# This will build an SIC frequency table
london.firms %>% 
  group_by(SICCode.SicText_1) %>% 
  summarise(n=n()) %>% 
  arrange(-n) %>% 
  glimpse()
```

## Clusters in space {.scrollable}

-   Focus on, let's say "70221 - Financial management"

```{r, echo=TRUE}
london.firms.sample <- london.firms %>% 
  filter(SICCode.SicText_1=="70221 - Financial management") %>% 
  select(oseast1m, osnrth1m) %>% 
  drop_na() 
```

## Financial management in London

```{r, echo=TRUE}
plot(london.firms.sample)
```

## Clusters in space, *k*-means

```{r, echo=TRUE}
fviz_nbclust(
  london.firms.sample, 
  kmeans, 
  k.max = 10,
  method = "wss"
)
```

## Clusters in space, *k*-means

```{r, echo=TRUE}
sp.cluster = kmeans(london.firms.sample, 6) 

plot(london.firms.sample, col = sp.cluster$cluster)
```

## Clusters in space, hdbscan

1.  Transform the space according to the density/sparsity

2.  Build the minimum spanning tree of the distance weighted graph

3.  Construct a cluster hierarchy of connected components

4.  Condense the cluster hierarchy based on minimum cluster size

5.  Extract the stable clusters from the condensed tree.

Resources: [SciKit-learn docs](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html){target="_blank"} and [dbscan package](https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html){target="_blank"}

## Clusters in space, hdbscan

```{r, echo=TRUE}
cl <- hdbscan(london.firms.sample, 
              minPts = 10)         #minimum size of clusters

plot(london.firms.sample, col=cl$cluster+1, pch=20)
```

## References
