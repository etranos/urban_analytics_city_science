---
title: "Diversity in cities"
author: |
  | 
  | Emmanouil Tranos
  |
  | University of Bristol, Alan Turing Institute 
  | [e.tranos@bristol.ac.uk](mailto:e.tranos@bristol.ac.uk), [\@EmmanouilTranos](https://twitter.com/EmmanouilTranos),  [etranos.info](https://etranos.info/)
format: revealjs
output:
  html_document:
    code_folding: hide
editor: source
bibliography: references.bib
---


## What is Diversity in Cities?


## What is Diversity in Cities?

- People > Population Diversity

- Firms, businesses & institutions > Economic Diversity

- Urban landscape > Morphological Diversity

- Animals & Plants > Species diversity

## What is Diversity in Cities?

- In general: ‘The state of being diverse‘

- With Spatial context: ‘The state of being diverse at one location OR throughout a geographical area

- In context of cities: ‘The state of being divrse within within and between urban places’ 



## Why is diversity important?

- concentration of diverse entities (people, firms, and other) at location promotes *creativity* and *innovation*


## In practice

1. Concentration/number & lack thereof

2. Spread - homogeneity and heterogeneity

3. Spillover - Geographical relation to concetration results in 

## General diversity measures

For example Spieces richness...

-   ... aka variety

-   $D = \sum_{i}^n p_{i}^0$

-   $p_i$ is the proportion of data points in the $i$th category

-   $n$ is the number of total categories

-   A count of different species / categories / ...

**Interpretation:**

-   Plurality

-   Availability of options


## General diversity measures

OR Shannon entropy

-   $H = -\sum_{i}^n p_{i} \ln{p_{i}}$

-   $n$ is the number of total categories

-   $p_i$ is the proportion of data points in the $i$th category

-   Probably the most common diversity index.

-   **Interpretation:**

    -   If one category dominates ➔ less surprise ➔ low entropy

    -   No category dominates ➔ more surprise ➔ high entropy

## Economic Diversity

 - High concentration, high diversity promotes collaboration and allows for economies of scale and economic growth

![](images/scale_economies.png){width=80%}

## Economic Diversity

- Marshalllian externalities - benefits gained from geographical agglomeration
  
  For example: Knowledge spillover, production spillover, ...

- Jacobian externalities - benefits gained from the diversity of economic activities within geography

  For example: Knowledge concentration correlated with production concentration
  
  
## Knowledge spillovers

Example of method: Spatial weights

![](images/sp_weight1.png){width=40%}

![](images/sp_weights2.png)

From @rowe_spatial_2021 and @li_new_2019

## Population Diversity

Cities are generators of cosmopolitanism

- ‘cosmos’ + ‘polis’ 

- ‘world’ + ‘city’ 

- city of the world

- cosmopolite = citizen of the word

- cosmoplitan = being part of the world, free from local attachments and prejudices

## Population Diversity

Chicago School - mosaic - spatial ecology

massive number of segregation studies > 'The ethnic city'

https://interactive.wttw.com/firsthand/segregation/mapping-chicago-racial-segregation


## Population Diversity

Later on shift >

- exchancge
- convivality
- multiculture
- spaces of difference
- engaging strangers

## Population Diversity

Steven Vertovec - Diversity and Contact 

> watch Stevens' lecture on Diversity -  https://www.youtube.com/watch?v=n7hKmjXcsJg&t=361s&ab_channel=DesignServer

Diversity is not just about the 'cosmo'

It can have negative effects such as 'halo effect' = xenofobic populism is highest in areas close to highly diverse or changing areas


## Mapping Diversity

1. Plotting the diversity metrics (shannon entropy, rates,...)

2. Clustering

## Clustering

![](images/ML_dis.png)

## Clustering

-   Reducing the dimensions of the observation space

-   Classification of observations into (exclusive) groups

-   Distance or (dis)similarity between each pair of observations to create a distance or dissimilarity or matrix

-   Observations within the same group are as similar as possible

-   Based on @boehmke2019hands available [here](https://bradleyboehmke.github.io/HOML/){target="_blank"}

-   Plenty of other resources online and in textbooks


## Clustering

1. Hierarchical

2. *k* means 

3. dbscan



## Clustering

**Hierarchical clustering**

::: columns
::: {.column width="60%"}
![](images/dendrogram.png)

<small> Source: [\@boehmke2019hands](https://bradleyboehmke.github.io/HOML){target="_blank"} </small>
:::

::: {.column width="40%"}
1.  Agglomerative clustering (AGNES -- AGglomerative NESting)

2.  Divisive hierarchical clustering (DIANA -- DIvise ANAlysis)

Dissimilarity (distance) of observations
:::
:::



## Clustering

**K-Means**

::: r-fit-text
1.  *k* is the number of clusters and is pre-defined

2.  The algorithm selects *k* random observations (starting centres)

3.  The remaining observations are assigned to the nearest centre

4.  Recalculates the new centres

5.  Re-check cluster assignment

6.  Iterative process to minimise *within-cluster variation* until convergence
:::

$SS_{within} = \sum_{k=1}^k W(C_{k}) = \sum_{k=1}^k \sum_{x_i\in C_K}(x_i-\mu_k)^2$




## Clustering 

**K-Means** in practice


stats::kmeans(x, centers = k, iter.max = 10, nstart = 1,
                algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"))
 

## How to choose *k* ?

1. The *elbow* method

2. Silhouette score/coefficient

3. Gap statistics


## How to choose *k* ?

1.  The *elbow* method

    -   Compute *k*-means clustering for different values of *k*

    -   Calculate $SS_{within}$ - the sum of square distances between the centroids and each points.


## How to choose *k* ?

2. Silhouette score
  
  - is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)
  
  - ranges from −1 to +1


## How to choose *k* ?

3. Gap statistics 

  - metric that describes how compact the clusters are > minimization problem

  - computes all the pairwise distances between points within a cluster and average these distances
  
  - read the original paper @tibshirani_estimating_2001
  
  
## How to choose *k* ?
    
![](images/cluster_choice.png)
From the optional practical on clustering

## Clustering 

**dbscan** or **hdbscan**

- identifies cluster by the density of the points

1. for each point constructs buffer with radius *r*

2. Counts all the other points within each buffer = *N* > Core points

3. Keep constructing buffers to points within the first buffer > iterates

4. Stops when it cannot expand any more


## Clustering 

**dbscan** or **hdbscan**

Resources: [SciKit-learn docs](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html){target="_blank"}, [dbscan package](https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html){target="_blank"}, [Youtube video]{https://www.youtube.com/watch?v=RDZUdRSDOok&ab_channel=StatQuestwithJoshStarmer}, [example K-means vs DBscan]{https://rpubs.com/datalowe/dbscan-simple-example}


## Population Diversity

Today the field is more concerned about the process of diversification.

> How are diverse environment created?

- ‘Route-ines’ are patterns of encounter that arise from fleeting interactions

 - Through 'rout-ines' people observe changes in their neighbourhoods and became more familiar with the people around them

Based on @vertovec_route-ines_2015

## Population Diversity

 1. Rooms without walls - urban spaces where interaction create social spaces and communities > patterns of social interactions
 
 2. Corridors of dissociation - urban places which are not where people are banned to interact in either by someone, institution or by themselves > patterns of social exclusion

Based on @vertovec_route-ines_2015



## Try out clustering yourself

* Optional practical on [github]{https://github.com/etranos/urban_analytics_city_science/tree/local/src/P8_clustering.Rmd}

* [K-means vs DBscan]{https://rpubs.com/datalowe/dbscan-simple-example}

* [Hierarchical & K-means clustering in R]{https://rpubs.com/pjmurphy/599072}




<!--
```{}
#| label: setup
#| echo: true
#| results: false
#| include: false

library(rprojroot)

# This is the project path
path <- find_rstudio_root_file()
images.path <- paste0(path, "/images/")
```

## Economic diversity

-   Production, i.e. firms

-   Consumption, i.e. product variety

-   Labour pool, i.e. skills in labour market

In general is *a good thing* for:

-   urban economies

-   productivity

-   urban and industrial agglomeration

## Opposing forces

-   Within-sector or Marshall--Arrow--Romer (MAR) spillovers

-   Between-sector or Jacobs spillovers

-   Large empirical literature trying to identify the optimal ratio, e.g. @saviotti2008export and @caragliu2016both

-   MAR externalities (or spillovers): good for productivity and short-term growth

-   Jacobean externalities: good for innovation and long-term growth

## Opposing forces

Using more clear economics terminology [@fujita1989urban]:

-   Diverse cities (heterogeneous agglomerations) enjoy economies of scope

-   Homogeneous agglomeration enjoy increasing returns from economies of scale

## On the ground

-   Ambiguous concepts

-   Variety, diversity, difference: a relative concept of agglomeration and the clustering of activities

-   Not only higher 'abundance', 'difference' or 'number', but also the degrees of 'richness', 'concentration'
or 'evenness' [@yuo2021environmental]

-   Different ways to measure [@bettencourt2021introduction]

## Spieces richness...

-   ... aka variety

-   $D = \sum_{i}^n p_{i}^0$

-   $p_i$ is the proportion of data points in the $i$th category

-   $n$ is the number of total categories

-   A count of different species / categories / ...

**Interpretation:**

-   Plurality

-   Availability of options

## Shannon entropy

-   $H = -\sum_{i}^n p_{i} \ln{p_{i}}$

-   $n$ is the number of total categories

-   $p_i$ is the proportion of data points in the $i$th category

-   Probably the most common diversity index.

-   **Interpretation:**

    -   If one category dominates ➔ less surprise ➔ low entropy

    -   No category dominates ➔ more surprise ➔ high entropy

## Herfindahl-Hirschman index

-   $HHI = \sum_{i}^{n}(p_{i}^2)$

-   $p_i$ is the proportion of data points in the $i$th category

-   Concentration of the market.

-   **Interpretation:**

    -   $1/n \leq HHI \leq 1$

    -   Two scenarios:

```{, echo=TRUE}
HHI_1 = .8^2 + .05^2 + .05^2 + .1^2
HHI_1

HHI_2 = .25^2 + .25^2 + .25^2 + .25^2
HHI_2
```

## Herfindahl-Hirschman index

-   Caution: alternative specification

-   $HHI = 1- \sum_{i}^{n}(p_{i}^2)$

# Examples

------------------------------------------------------------------------

![](images/paper_example1.png)

------------------------------------------------------------------------

![](images/paper_example2.png)

## Relatedness

-   *Relatedness* spans the continuum between MAR and Jacobs [@hidalgo2021economic]

-   Related activities are neither exactly the same nor completely different [@frenken2007related; @boschma2012technological]

-   Why? Because:

    -   identical activities compete for customers and resources,

    -   no learning between very dissimilar economic activities

## Relatedness

-   Absorptive capacity: a firm's capacity to absorb new knowledge depends on its prior level of related knowledge [@cohen1990absorptive]

## Economic complexity

-   Large scale fine-grained data on economic activities

-   Learn about abstract factors of production and the way they combine into outputs

-   Dimensionality reduction techniques to data on the geography of activities, e.g. employment by industry or patents by technology

-   Machine learning and network techniques to predict and explain the economic trajectories of countries, cities and regions

For a review, check @hidalgo2021economic and @balland2022new.

## Measuring diversity

<iframe src="https://www.gov.uk/government/organisations/companies-house" width="800" height="600" frameBorder="0" scrolling="yes">

</iframe>

## Measuring diversity

-   Go to [data.london.gov.uk](https://data.london.gov.uk/dataset/directory-of-london-businesses){target="_blank"}

-   Download and and save locally the `Businesses-in-London.csv`

-   Make sure you know the file location!

-   We will use the `REAT` and `entropy` packages. Check what these packages do [here](https://cran.r-project.org/web/packages/REAT/REAT.pdf){target="_blank"} and
[here](https://cran.r-project.org/web/packages/entropy/entropy.pdf){target="_blank"}.

-   Install them if needed with `install.packages("packagename")`

## Measuring diversity {.scrollable}

```{, echo=TRUE}

library(tidyverse)  # for data wrangling
library(rprojroot)  # for relative paths
library(REAT)       # for diversity measures
library(entropy)    # for entropy
library(cluster)    # for cluster analysis
library(factoextra) # help functions for clustering 
library(kableExtra) # for nice html tables
library(dbscan)     # for HDBSCAL
library(sf)         # for mapping

# This is the project path
path <- find_rstudio_root_file()
path.data <- paste0(path, "/data/businesses-in-london.csv")

london.firms <- read_csv(path.data) 

london.firms.sum <- london.firms %>% 
  filter(SICCode.SicText_1!="None Supplied") %>% # dropping NAs in essence
  group_by(oslaua, SICCode.SicText_1) %>%        # grouping by Local Authority and SIC code
  summarise(n = n()) %>%                         # summarise: n is the number of firms per Local Authority and SIC code
  mutate(total = sum(n),                         # total equal all firms
         freq = n / total) %>%                   # just a frequency
  group_by(oslaua) %>%                           # grouping again only by Local Authority
  summarise(richness = n_distinct(SICCode.SicText_1), # the number of distinct SIC per Local Authority
            entropy = entropy(freq, method = "ML"),   # entropy for each Local Authority, we did the first group_by() and mutate() to be                                                           able to calculate freq so we can calculate entropy
            herf = herf(n)) %>%                       # HHI for each local authority
  arrange(-herf)                                      # sort based on HHI (descending)

london.firms.sum %>% kbl() %>%
  kable_styling(full_width = F) %>%                   # A nice(r) table
  scroll_box(width = "800px", height = "300px")
```

## Measuring diversity

::: callout-tip
You don't know what local authorities these codes refer to. You should download the codes and names and join them with your data from [here](https://geoportal.statistics.gov.uk/datasets/ons::local-authority-districts-december-2021-uk-bfe-1/explore){target="_blank"}. 
:::

::: callout-tip
Discuss what we can learn from this exercise.

Can you think of a way to understand how different these indices are among London's Local Authorities?
:::

## Mapping diversity {.scrollable}

```{, }
#| label: maps
#| echo: true
#| include: true
#| message: false
#| warning: false
#| error: false
#| results: asis

path.shape <- paste0(path, "/data/Local_Authority_Districts_(May_2021)_UK_BFE.geojson")

london <- st_read(path.shape, quiet = T) %>%
  dplyr::filter(LAD21CD %in% (london.firms$oslaua))

london <- merge(london, london.firms.sum, by.x = "LAD21CD", by.y = "oslaua" )
  
ggplot() +
  geom_sf(data = london, aes(fill = entropy), color = NA) +
  labs(
    title = "Business diversity in London' Local Autorities",
    fill = "Entropy") +
  scale_fill_viridis_c() +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5)) # centres the title
```

## Clustering

-   Reducing the dimensions of the observation space

-   Classification of observations into (exclusive) groups

-   Distance or (dis)similarity between each pair of observations to create a distance or dissimilarity or matrix

-   Observations within the same group are as similar as possible

-   Based on @boehmke2019hands available [here](https://bradleyboehmke.github.io/HOML/){target="_blank"}

-   Plenty of other resources online and in textbooks

------------------------------------------------------------------------

![Source: [medium.com](https://medium.com/@recrosoft.io/supervised-vs-unsupervised-learning-key-differences-cdd46206cdcb)](images/classical_ml.png){target="_blank"}

## Clustering

1.  *k*-means

2.  Hierarchical clustering

## *k*-means

::: r-fit-text
1.  *k* is the number of clusters and is pre-defined

2.  The algorithm selects *k* random observations (starting centres)

3.  The remaining observations are assigned to the nearest centre

4.  Recalculates the new centres

5.  Re-check cluster assignment

6.  Iterative process to minimise *within-cluster variation* until convergence
:::

$SS_{within} = \sum_{k=1}^k W(C_{k}) = \sum_{k=1}^k \sum_{x_i\in C_K}(x_i-\mu_k)^2$

## *k*-means {.scrollable}

First, create an appropriate data frame

```{, echo = TRUE}

la.sic <- london.firms %>% 
  filter(SICCode.SicText_1!="None Supplied") %>% # Drop firms which haven't declared SIC code
  group_by(oslaua, SICCode.SicText_1) %>%        # Group by Local Authorities and SIC code
  summarise(n = n()) %>%                         # Summarise; n = number of observations
  mutate(total = sum(n),                         # New column: total number of observations
         freq = n / total) %>%                   # New column: frequency
  arrange(oslaua,-n) %>%                         # Just arrange by Local Authority and descenting order of n
  select(-n, -total) %>%                         # Drop n and total, we don't need them any more.
  pivot_wider(names_from = SICCode.SicText_1, values_from = freq) %>% # Data transformation: from long to wide. Have a look: https://tidyr.tidyverse.org/reference/pivot_wider.html
  replace(is.na(.), 0)                          # Replace any missing values with 0 as missing value represent SIC codes with 0 frequency

la.sic %>%  
  select(1:20) %>%  # Select the first 20 columns as there 1037 in total
  kbl() %>%
  kable_styling()   # Nice(r) table
```

## *k*-means {.scrollable}

```{, echo=TRUE}
kclust = kmeans(la.sic[,-1], centers = 10, nstart = 10) # be aware of the [,-1]
str(kclust)
```

`centers` is 10 x 1036: 1036 is the number of SIC codes.

## Choosing *k*

1.  Rule of thumb: $k = \sqrt{n/2}$

2.  The *elbow* method

    -   Compute *k*-means clustering for different values of *k*

    -   Calculate $SS_{within}$

    -   Plot and spot the loction of a bend

## Choosing *k*

```{, echo=TRUE}

fviz_nbclust(
  la.sic[,-1], 
  kmeans, 
  k.max = 20,
  method = "wss"
)

```

## Hierarchical clustering

::: columns
::: {.column width="60%"}
![](images/dendrogram.png)

<small> Source: [\@boehmke2019hands](https://bradleyboehmke.github.io/HOML){target="_blank"} </small>
:::

::: {.column width="40%"}
1.  Agglomerative clustering (AGNES -- AGglomerative NESting)

2.  Divisive hierarchical clustering (DIANA -- DIvise ANAlysis)

Dissimilarity (distance) of observations
:::
:::

## Hierarchical clustering

```{, echo=TRUE}

# distances between observations
d <- dist(la.sic)

# creates labels for the dendrogam
l <- london.firms %>% distinct(oslaua) %>% arrange(oslaua)

hclust = hclust(d)

plot(hclust, hang=-1, labels=l$oslaua, main='Default from hclust') 
#hang: the fraction of the plot height by which labels should hang below the rest of the plot. A negative value will cause the labels to hang down from 0.
```

## Optimal number of clusters

```{}
p1 <- fviz_nbclust(la.sic, FUN = hcut, 
                   method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(la.sic, FUN = hcut, 
                   method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(la.sic[-1], FUN = hcut, 
                   method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

## Optimal number of clusters

::: callout-tip
Explore what the 2 cluster solution tells us about London?
:::

```{, eval=FALSE}
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hclust)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = .15)
fviz_dend(dend_cuts$lower[[2]])

sub_grp <- cutree(hclust, k = 2)
table(sub_grp)

fviz_dend(
  hclust,
  k = 2,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco",
  cex = 0.1
)
```

## Clusters in space {.scrollable}

-   Create a SIC frequency table

```{, echo=TRUE}

# This will build an SIC frequency table
london.firms %>% 
  group_by(SICCode.SicText_1) %>% 
  summarise(n=n()) %>% 
  arrange(-n) %>% 
  glimpse()
```

## Clusters in space {.scrollable}

-   Focus on, let's say "70221 - Financial management"

```{, echo=TRUE}
london.firms.sample <- london.firms %>% 
  filter(SICCode.SicText_1=="70221 - Financial management") %>% 
  select(oseast1m, osnrth1m) %>% 
  drop_na() 
```

## Financial management in London

```{, echo=TRUE}
plot(london.firms.sample)
```

## Clusters in space, *k*-means

```{, echo=TRUE}
fviz_nbclust(
  london.firms.sample, 
  kmeans, 
  k.max = 10,
  method = "wss"
)
```

## Clusters in space, *k*-means

```{, echo=TRUE}
sp.cluster = kmeans(london.firms.sample, 6) 

plot(london.firms.sample, col = sp.cluster$cluster)
```

## Clusters in space, hdbscan

1.  Transform the space according to the density/sparsity

2.  Build the minimum spanning tree of the distance weighted graph

3.  Construct a cluster hierarchy of connected components

4.  Condense the cluster hierarchy based on minimum cluster size

5.  Extract the stable clusters from the condensed tree.

Resources: [SciKit-learn docs](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html){target="_blank"} and [dbscan package](https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html){target="_blank"}

## Clusters in space, hdbscan

```{, echo=TRUE}
cl <- hdbscan(london.firms.sample, 
              minPts = 10)         #minimum size of clusters

plot(london.firms.sample, col=cl$cluster+1, pch=20)
```

-->

## References
